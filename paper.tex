\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{RadicalSea: Improving data management in BigData pilot job applications on HPC}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
\end{abstract}

\begin{IEEEkeywords}
\end{IEEEkeywords}

\section{Introduction}
\begin{itemize}
\item Improving the performance of scientific big data applications is a growing 
    concern
\item Big Data frameworks and tools not instrumented for HPC (hpc relies on 
    shared parallel storage rather than distributed storage, schedulers employed 
are not Big Data schedulers, need overlay clusters)
\item HPC frameworks have been developed to addressed this (radical).
\item performance in Big Data applications is constrained by data movement
\item Shared filesystem i/o very costly, however users require data (sometimes 
    including intermediate data) to be stored there for future access.
\item overlay filesystems improve performance while only temporarily increasing
    storage bandwidth. Users have the flexibility to decide whether the overlay
    will provide improved performance to their pipelines.
\item Radical allows the offsetting of this cost by transferring intermediate
    data between nodes. Does not leverage local node's different storage layers
    and will not transfer the intermediate data back to shared filesystem.
\item Sea is a data management library, that, based on a list of user's storage 
    preference will perform i/o on fastest available storage while 
    asynchronously flushing all data to shared filesystem.
\item sea and radical can be hooked together to create an efficient big data
    engine for HPC
\item As RADICAL has knowledge of the pipeline internals, it can provide Sea
    with details on which data to ``pin'' to memory (through filename??)
\item When combined, sea and radical pilot should provide performance that is
    competitive to that of Big Data engines.
\item Goal: evaluate the added performance of sea with radical vs radical alone 
    vs Spark on HPC using two scientific big data workflows (BigBrain + ??)
\end{itemize}
\section{Materials and Methods}
\begin{itemize}
    \item{Radical Cybertools}
    \item{Sea}
       \\ - Sea is a library that attempts to mask read and write times of intermediary data to shared parallel
        filesystems by leveraging available node-local storage whenever possible. 
        \\ - In a config file, a user specifies a mount point (a directory for the application to interact with) and several sources
        (when an application accesses a file/directory located within the mountpoint, they will actually be interfacing with one/all of the sources)
        \\ - Sea leverages LD\_PRELOAD in order to intercept libc calls and redirect them to the appropriate sources. This method has limitations in that
        it will not work with statically-linked libraries or applications that do not interface with libc.
        \\ - Although Sea would not have the flexibility of filesystems such as FUSE-based systems, its performance is expected to be significantly faster than
    a FUSE-based implementation. (maybe more intro. should also discuss other LD\_PRELOAD projects (e.g. xtreemfs) , union mounts (unionfs), other popular examples that leverage LD\_PRELOAD-ing(fakechroot), big data fs (HDFS, alluxio)))
        \\ - The config file will also specific a source hierarchy such that Sea will attempt to perform I/O at the top of the hierarchy whenever possible,
        moving down the hierarchy as the top sources get filled up. Fast filesystems, such as tmpfs, should be located at the top of the hierarchy, whereas slower
        fs, such as shared parallel fs should be located at the bottom. 
        \\ - The ``slowest'' filesystem will be the one in while all intermediary data is materialized to.
        \\ - In order to free space at the top of the hierarchy, flusher and eviction threads will be started by Sea. For now, implement a simple LRU-based policy. (Ideally, flusher tasks will coincide with compute tasks such that flushing time does not impact I/O)
        \\ - All data is evicted to permanent storage before node can be relinquished
        \\ - Sea brings added performance to workflows with non-negligible compute time, as the flush time to Lustre will not be successfully masked in strictly data intensive applications.

    \item{Radical-Sea Interface}
        \\ - Standalone, Sea can only operate on a single node. To leverage Sea in a distributed environment, Sea needs to be launched in all nodes started by the
        resource manager.
        \\ - RADICAL can ensure that Sea is running within each allocated node prior to task execution
        \\ - data-locality needs to be preserved to benefit from Sea optimizations. In the instance where a Sea-generated file has not yet been flushed to shared storage, RADICAL-PILOT can issue a notice to the SEA flushers (touchfile perhaps?) to force flushing.
        \\ RADICAL can also provide to SEA the DAG of dependencies such that SEA can avoid evicting files which will be reused.
    \item{Infrastructure} \\our cluster, BRIDGES
    \item{Datasets} \\ bigbrain 603GB + Geo dataset
    \item{Experiments}
        \\ - Experiment 1 - Synthetic pipeline leveraging real data (BigBrain/Neuroimaging). The goal of this experiment is to determine
            where RadicalSea ranks in comparison to the worst case (writing to shared fs) and best case (writing to memory only) on
            a data intensive pipeline operating on a very large dataset. 
            We will increase the amount of intermediary data produced from 1 - 10 to see how RADICALSea will perform with increasing amount of data generated.
            We are not necessarily interested in the read/write time of individual
        \\      - 1.1 Radical pipeline + shared fs
        \\      - 1.2 Radical pipeline + local disk (or mem if there's enough memory)
        \\      - 1.3 Spark implementation
        \\      - 1.4 RadicalSea
        \\
\end{itemize}
\section{Results}
\section{Conclusion}

\section*{Acknowledgment}

\section*{References}


\end{document}
