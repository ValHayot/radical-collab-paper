\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{RadicalSea: Improving data management in BigData pilot job applications on HPC}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
\end{abstract}

\begin{IEEEkeywords}
\end{IEEEkeywords}

\section{Introduction}
\begin{itemize}
\item Improving the performance of scientific big data applications is a growing 
    concern
\item Big Data frameworks and tools not instrumented for HPC (hpc relies on 
    shared parallel storage rather than distributed storage, schedulers employed 
are not Big Data schedulers, need overlay clusters)
\item HPC frameworks have been developed to addressed this (radical).
\item performance in Big Data applications is constrained by data movement
\item Shared filesystem i/o very costly, however users require data (sometimes 
    including intermediate data) to be stored there for future access.
\item overlay filesystems improve performance while only temporarily increasing
    storage bandwidth. Users have the flexibility to decide whether the overlay
    will provide improved performance to their pipelines.
\item Radical allows the offsetting of this cost by transferring intermediate
    data between nodes. Does not leverage local node's different storage layers
    and will not transfer the intermediate data back to shared filesystem.
\item Sea is a data management library, that, based on a list of user's storage 
    preference will perform i/o on fastest available storage while 
    asynchronously flushing all data to shared filesystem.
\item sea and radical can be hooked together to create an efficient big data
    engine for HPC
\item As RADICAL has knowledge of the pipeline internals, it can provide Sea
    with details on which data to ``pin'' to memory (through filename??)
\item When combined, sea and radical pilot should provide performance that is
    competitive to that of Big Data engines.
\item Goal: evaluate the added performance of sea with radical vs radical alone 
    vs Spark on HPC using two scientific big data workflows (BigBrain + ??)
\end{itemize}
\section{Materials and Methods}
    \subsection{Radical Cybertools}
    \subsection{Sea}
        \begin{itemize}
                \item Sea is a library that attempts to mask read and write times of intermediary data to shared parallel
        filesystems by leveraging available node-local storage whenever possible.
                \item In a config file, a user specifies a mount point (a directory for the application to interact with) and several sources
        (when an application accesses a file/directory located within the mountpoint, they will actually be interfacing with one/all of the sources)
                \item Sea leverages LD\_PRELOAD in order to intercept libc calls and redirect them to the appropriate sources. This method has limitations in that
        it will not work with statically-linked libraries or applications that do not interface with libc.
                \item  Although Sea would not have the flexibility of filesystems such as FUSE-based systems, its performance is expected to be significantly faster than
    a FUSE-based implementation. (maybe more intro. should also discuss other LD\_PRELOAD projects (e.g. xtreemfs) , union mounts (unionfs), other popular examples that leverage LD\_PRELOAD-ing(fakechroot), big data fs (HDFS, alluxio)))
                \item  The config file will also specific a source hierarchy such that Sea will attempt to perform I/O at the top of the hierarchy whenever possible,
        moving down the hierarchy as the top sources get filled up. Fast filesystems, such as tmpfs, should be located at the top of the hierarchy, whereas slower
        fs, such as shared parallel fs should be located at the bottom. 
                \item The ``slowest'' filesystem will be the one in while all intermediary data is materialized to.
                \item In order to free space at the top of the hierarchy, flusher and eviction threads will be started by Sea. For now, implement a simple LRU-based policy. (Ideally, flusher tasks will coincide with compute tasks such that flushing time does not impact I/O)
                \item All data is evicted to permanent storage before node can be relinquished
                \item Sea brings added performance to workflows with non-negligible compute time, as the flush time to Lustre will not be successfully masked in strictly data intensive applications.
        \end{itemize}
    \subsected{RadicalSea interface}
    \begin{itemize}
            \item Standalone, Sea can only operate on a single node. To leverage Sea in a distributed environment, Sea needs to be launched in all nodes started by the
        resource manager.
            \item RADICAL can ensure that Sea is running within each allocated node prior to task execution
            \item data-locality needs to be preserved to benefit from Sea optimizations. In the instance where a Sea-generated file has not yet been flushed to shared storage, RADICAL-PILOT can issue a notice to the SEA flushers (touchfile perhaps?) to force flushing.
            \item RADICAL can also provide to SEA the DAG of dependencies such that SEA can avoid evicting files which will be reused.
    \end{itemize}
    \subsection{Infrastructure}
    \begin{itemize}
            \item slashbin cluster
            \item bridges
    \end{itemize}
    \subsection{Datasets}
    \begin{itemize}
            \item bigbrain 603GB
            \item Iceberg dataset (4TB??)
    \end{itemize}
    \subsection{Experiments}
    \subsubsection{Experiment 1 - BigBrain}
    \begin{itemize}
            \item Synthetic pipeline leveraging real data (BigBrain/Neuroimaging). The goal of this experiment is to determine
            where RadicalSea ranks in comparison to the worst case (writing to shared fs) and best case (writing to memory only) on
            a data intensive pipeline operating on a very large dataset. 
            We will increase the amount of intermediary data produced from 1 - 10 to see how RADICALSea will perform with increasing amount of data generated.
            We are not necessarily interested in the read/write time of individual i/o operations, but moreso at the pipeline task level (e.g. is flushing to lustre occurring at
            the same time as compute therefore being masked or is it actually now worse than writing directly to shared fs.
            \item Radical pipline + shared fs
            \item Radical pipeline + local disk(or mem)
            \item Spark implementation
            \item Radical Sea
    \end{itemize}
    \subsubsection{Experiment 2 - Iceberg data}
    \begin{itemize}
        \item Real pipeline and real data experiment. Here we are looking at how RadicalSea performs with variable task duration. Also interested in makespan of application/percentage of lustre  i/o being masked and how that compares to worst and best case.
        \end{itemize}

\section{Results}
\section{Conclusion}

\section*{Acknowledgment}

\section*{References}


\end{document}
